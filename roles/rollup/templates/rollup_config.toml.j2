{{ rollup_da_config }}

[storage]
path = "{{ rollup_storage_dir }}"
# TODO: this to variable
state_cache_size = 4294967296 # 4GB. Default is 1GB.
user_commit_concurrency = 6 # Number of concurrent commit workers for user state
# TODO: expose in variable config
user_hashtable_buckets = 10_000_000 # Expected state size, cannot be changed after creation
#user_preallocate_ht = false # Whether to preallocate hashtable file for user db
# user_page_cache_size = 1073741824 # Page cache size for user state in bytes
# user_leaf_cache_size = 536870912 # Leaf cache size for user state in bytes
kernel_commit_concurrency = 2 # Number of concurrent commit workers for kernel state
kernel_hashtable_buckets = 256000 # Hashtable buckets for kernel db
# kernel_preallocate_ht = false # Whether to preallocate hashtable file for kernel db
# kernel_page_cache_size = 268435456 # Page cache size for kernel state in bytes
# kernel_leaf_cache_size = 134217728 # Leaf cache size for kernel state in bytes
pruner_block_interval = 100 # How often to run pruner measured by DA block
pruner_versions_to_keep = 20 # Number of versions available for historical querying
# pruner_max_batch_size = 300000 # The maximum number of key/value pairs to prune in a single batch. Each key is about 70 bytes, so your prunery memory requirements will be about 21MB by default.


[runner]
# TODO: Define this variable in da
da_polling_interval_ms = 2000
da_total_timeout_secs = 300
concurrent_sync_tasks = 2

[runner.http_config]
bind_host = "{{ rollup_http_host }}"
bind_port = {{ rollup_http_port }}

[monitoring]
telegraf_address = "127.0.0.1:8094"

[proof_manager]
aggregated_proof_block_jump = 1
# For now sequencer and prover same thing
prover_address = "{{ rollup_sequencer_address }}"
max_number_of_transitions_in_db = 100
max_number_of_transitions_in_memory = 20

[sequencer]
# This value is counted between sequencer receiving receipt from DA about inclusion till node has processed this blob.
blob_processing_timeout_secs = 600
max_batch_size_bytes = 1048576
max_concurrent_blobs = 128
max_allowed_node_distance_behind = 10
rollup_address = "{{ rollup_sequencer_address }}"
[sequencer.preferred]
# TODO: Remove it when https://github.com/Sovereign-Labs/sovereign-sdk-wip/issues/2814 is resolved
disable_state_root_consistency_checks = true
# Strategy for handling recovery scenarios when the sequencer is too far behind.
# "None" - Shutdown the sequencer instead of attempting recovery (default)
# "TryToSave" - Attempt to recover by flushing batches and catching up with the chain
recovery_strategy = "TryToSave"
# TODO: Set this by mock_da
batch_execution_time_limit_millis = 12000 # Do no more than 12 seconds worth of work per batch, even if the DA layer isn't producing blocks regularly
# The sequencer optimistically pre-executes transactions across multiple worker threads.
# This warms up caches so the main transaction executor can run with ready-to-use data.
# This variable determines the number of worker threads.
num_cache_warmup_workers = 0
is_replica = {{ is_rollup_replica | lower }}
{% if rollup_sequencer_postgres_connection_string is defined and rollup_sequencer_postgres_connection_string | length > 0 %}
postgres_connection_string = "{{ rollup_sequencer_postgres_connection_string }}"
{% endif %}
[sequencer.extension]
max_log_limit = 20000